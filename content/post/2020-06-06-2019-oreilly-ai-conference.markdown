---
title: 2019 Oreilly AI Conference
author: Govind G Nair
date: '2020-06-06'
slug: 2019-oreilly-ai-conference
categories:
  - Conferences
  - Machine Learning
  - AI
tags:
  - Machine Learning
  - Strategy
subtitle: ''
summary: 'Notes from the 2019 Oreilly AI Conference'
authors: []
lastmod: '2020-06-06T12:35:29-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

These are the notes covering my learning from the talks at the O'Reilly Artificial Intelligence Conference held in San Jose in 2019.There are over a hundred hours of content from this conference so I will be continually updating this post as I consume it.

Also note that, I have only included content that was new or interesting to me. It is not a comprehensive report on these talks.


## 1) Fighting Crime with Graphs | (MIT + IBM)

Use of graphs to fight financial crime has been getting a lot of traction recently. This talk emphasized the use of Graph Convolutional Networks to create node embeddings that can be consumed by traditional machine learning models or standard neural nets.

Algorithms like node2vec and deepwalk have been used to create embeddings that capture the topology of a network, but these algorithms cannot capture the node attributes that might have rich information.

GraphSage is a Graph convolutional network algorithm that allows you to capture both the topology of a network as well as useful node attributes.Besides this is an **inductive"" algorithm meaning that it does not need to be trained on whole graphs and can be used for inference on unseen nodes and graphs.

More useful information is available [here](http://snap.stanford.edu/graphsage/) and [here](https://blogs.oracle.com/datascience/graphwise-graph-convolutional-networks-in-pgx)


## 2) Facebook Keynote -  Going beyond supervised learning


Broad classes of Machine Learning outside the classical supervised approaches used at Facebook:

a) Weak Supervision: Use labels are already available in the data e.g. hashtags in Instagram photos. Facebook built the world's best image recognition system using the 3.5 billion images shared on Instagram using the hashtags.

A state of the art model was also created on 65 million videos to recognize 10,000 actions using the available hash tags.

b) Semi Supervised Learning: Use a small amount of labelled data to train a teacher model model and use this model to generate predictions on unlabeled data. Now pre-train a student model on this larger data set and fine tune on the original labelled data.


c) Self Supervised Learning: The best example is Language modelling where you can take a sentence of  length N and use the first N-1 words to predict the Nth word or predict a word in the middle using the surrounding words.

At facebook, self supervised learning based on Generative Adversarial Networks is used to create more robust models that can be used to detect images that have been manipulated by overlaying patterns to get around restrictions such as nudity.

This is done by training a GAN to recover the  original image from a manipulated image.

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/fb_self_supervised_GAN.PNG)

FB takes images, overlays it with random patterns. The generator tries to recreated the original while the discriminator tries to discriminate between the originals and the image generated by the generator. The model minimizes the adversarial loss which captures how well the system can fool the discriminator as well as the perceptual loss which captures how well the generator is able to recreate the original.

After training, just the generator can be used to remove patterns from images.

Another interesting application of this technique at Facebook is **machine translation without labels**.

This relies on creating automatic word dictionaries. First,you create vector representations for vocabularies in each languages and leverage the fact that the vector representations of the same word in different languages share structure to learn a *rotation matrix* to align them and produce a word by word translation.

Then  you build a language model in each language to learn the sentence structure in each language. This language model can then be used to reorder the word by word translation produced in the previous step. This gives you an initial model to translate between the two languages (L1 to L2 * L2 to L1).

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/Language_Model0.PNG)

Now a technique called **back translation** can be used to iteratively improve the model. In this method, you take the original sentence in Language 1 and translate this to Language 2 using the initial *L1 to L2*  model. Now you translate this back to language 1  using the *L2 to L1* model with the original sentence in Language 1 being the target. This will allow you to improve the *L2 to L1* model. By flipping the order of the translations, you can also improve the *L1 to L2* model

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/Language_Model1.PNG)


d) Reinforcement Learning:

Facebook uses RL to recommend notifications to users on Instagram. 

FB also has created a platform called [Habitat](https://aihabitat.org/) 'a research platform for embodied AI agents to operate in real world-like environments and solve tasks like navigation, question-answering, et cetera'.


## 3) Data Science + Design Thinking  - A Perfect blend to achieve the best user experience  | Intuit

Design for delight by
1) Demonstrating deep customer empathy - Know customers deeply by observing them and define problems in human centric terms
2) Go broad to go narrow - Generate lots of ideas before winnowing them. Quantity first then focus on quality.
3) Perform rapid experiments with customers - Rapid prototyping and AB testing


## 4) Explaining Machine Learning Models | Fiddler Labs

Attribution problem : Attribute a prediction to input features. Solving this is the key goals of Explaining ML Models

Naive approaches to do this include:

1) Ablation: Drop each feature and note the change in prediction
2) Feature Gradient: $ x_i \times  \frac {dy}{dx_i}$ where $ x_i $ is feature and $ y $ is the output

**Integrated Gradients** -  This is a technique for attributing a differentiable model's prediction to the input features

A popular method for non-differentiable models is Shapley values. 


Both Integrated Gradients and Shapley Values come with some axiomatic guaranteed.The former uniquely satisfies 6 axioms while the latter uniquely satisfies 4. Side note: [This](https://christophm.github.io/interpretable-ml-book/)
is my go to reference for interpretability techniques.


Example of an axiom is the sensitivity axiom:

> All else being equal, if changing a feature changes the output, then that feature should get an attribution. Similarly if changing a feature does not change the output, it should not get an attribution.

Integrated Gradients is the unique path integral method that satisfies: Sensitivity, Insensitivity, Linearity preservation, Implementation invariance, Completeness and Symmetry


Another problem related to interpretability that remains an open problem for many classes of black box models is 
**Influence** - i.e. Which data points in the training data influenced the model the most.



## 5) Snorkel

Snorkel is a weak supervised learning approach that came out of Stanford. More information available [here](https://www.snorkel.org/)

The key operations in the Snorkel workflow include:

1) Labeling Functions: Heuristics to label data provided by experts
2) Transformation Functions: Data Augmentations
3) Slicing Functions: Partition the data specifying critical subsets where model performance needs to be high

For this approach to work at least 50% of the labeling functions need to be better than random.


## 6) Managing AI Products | Salesforce

To demonstrate value to business stakeholders, which is the ultimate goal of anyone who works in a corporation, it is essential to tie business metrics to model metrics. This should ultimately inform what kind of ML we decide to use.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/acceptance_criteria.PNG)

The figure above demonstrates the accuracy of the model(x-axis) required to provide a material lift in the business metric(y-axis) e.g. conversion rate. If the base line improvement rate in conversion that we need to deliver is only 2%, a model that has accuracy in the range 50 - 75% is sufficient. This means we could rule out sophisticated models like Neural Nets that are harder to deploy and maintain and focus on simpler models that are easier to build or maintain. 


## 7) Explainability and Bias in AI and ML| Institute for Ethical AI and ML

Undesired bias can be split into two conceptual pieces

**1) Statistical Bias (Project Bias)** : The error between where you ARE and where you could get caused by modeling/project decisions

  * Sub optimal choices of accuracy metrics/cost functions
  * Sub optimal choices of ML models chosen for the task
  * Lack of infrastructure required to monitor model performance in production
  * Lack of human in the loop where necessary



**2) A-priori bias (Societal Bias)** : The error between the best you can practically get, and the idealistic best possible scenario - caused by a-priori constraints

  * Sub optimal business objectives
  * Lack of understanding of the project
  * Incomplete resources (data, domain experts etc)
  * Incorrectly labelled data (accident or otherwise)
  * Lack of relevant skill sets
  * Societal shifts in perception
  

Explainability is key to:

a) Identify and evaluate undesirable biases
b) To meet regulatory requirements such as GDPR
c) For compliance of processes
d) To identify and reduce risks (FP vs FN)

**Interpretability != Explainability**

 * Having a model that can be interpreted doesn't mean in can be explained
 * Explainability requires us to go beyond algorithms
 * Undesired bias cannot be tackled without explainability

Library for Explainable AI: [xAI](https://github.com/EthicalML/XAI) [alibi](https://github.com/SeldonIO/alibi)



Anchor points: What are features that influenced a specific prediction for a data instance? This can be evaluated by roughly by pulling out a feature and estimating its impact on the model prediction.

Counterfactual: How would the input/features have to change for the prediction to change?


## 8) Usable Machine Learning - Lessons from Stanford and beyond | Stanford University

- For deep learning, improvement in performance requires exponential increase in data
- Deep learning still doesn't work very well with structured data
- Don't look for a perfect model right out of the gate, instead iterate towards higher quality models
- Measure implicit signals where possible. e.g. Is a user spending time on a page or closing a window


## 9) Human Centered Machine Learning | H2O.ai

- Establish a benchmark using a simple model from which to gauge improvements in accuracy, fairness, interpretability or privacy

-  Overly complicated featured are hard to explain. Features should provide business intuition.(More relevant for regulated industries)

- For fairness, it is important to evaluate if different sub groups of people are being treated differently by your ML model (Disparate Impact). Need to do appropriate data processing. OSS:  [AIF360](https://github.com/IBM/AIF360), [aequitas](https://github.com/dssg/aequitas)

- Model Debugging for Accuracy, Privacy or Security: This involves eliminating errors in model predictions by testing using adversarial examples, explaining residuals, random attacks and what if analysis. Useful OSS: [cleverhans](https://github.com/tensorflow/cleverhans), [pdpbox](https://github.com/SauceCat/PDPbox),
[what-if-tool](https://github.com/pair-code/what-if-tool)


## 10) Operationalizing AI at Scale: From drift detection to monitoring business impact | IBM Watson

Note: See my post on [concept drift](https://www.govindgnair.com/post/detecting-concept-drift/) for a general understanding of the problem.


* Fair Credit Act  requires a sample of model decisions to be explained to the regulator. Same with GDPR.

Watson Openscale provides the following capabilities:

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Watson.PNG)

Important to realize high model performance doesn't always correlate to high business performance. Need to correlate model KPIs and Business KPIs. IBM openscale will automatically correlate model metrics in run time with business event data to inform impact on business KPIs.

Types of concept drift:

1) Class Imbalance. e.g. The imbalance ratio shifts.
2) Novel Class Emergence e.g. A new category of chats in a chatbot
3) Existing Class Fusion e.g. A bank wants to merge the loan approved class with the loan partially approved class

Accuracy Drift: Has model accuracy changes because of changing business dynamics? This is what businesses really care about. This could happen due to:

a) Drop in Accuracy: Model accuracy could drop if there is an increase in transactions similar to those which the model was unable to evaluate correctly in training data

- Use training and test data to learn what kind of data the model is making accurate prediction on and where it is not. You can build a secondary model to predict the model accuracy of your primary model and see if the accuracy falls

b) Drop in data consistency:  Drop in consistency of data at runtime compared to characteristics at running time.

E.g. % of married people applying for your loan has increased from 15% to 80%. These are the kind of explanations that are accessible to a business user.

- Open scale can map a drop in business KPI to the model responsible for the drop and identify the samples of data that has changed.

































