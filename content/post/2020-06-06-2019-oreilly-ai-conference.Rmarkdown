---
title: 2019 Oreilly AI Conference
author: Govind G Nair
date: '2020-06-06'
slug: 2019-oreilly-ai-conference
categories:
  - Conferences
  - Machine Learning
  - AI
tags:
  - Machine Learning
  - Strategy
subtitle: ''
summary: 'Notes from the 2019 Oreilly AI Conference'
authors: []
lastmod: '2020-06-06T12:35:29-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

These are the notes covering my learning from the talks at the O'Reilly Artificial Intelligence Conference held in San Jose in 2019.There are over a hundred hours of content from this conference so I will be continually updating this post as I consume it.

Also note that, I have only included content that was new or interesting to me. It is not a comprehensive report on these talks.


## 1) Fighting Crime with Graphs | (MIT + IBM)

Use of graphs to fight financial crime has been getting a lot of traction recently. This talk emphasized the use of Graph Convolutional Networks to create node embeddings that can be consumed by traditional machine learning models or standard neural nets.

Algorithms like node2vec and deepwalk have been used to create embeddings that capture the topology of a network, but these algorithms cannot capture the node attributes that might have rich information.

GraphSage is a Graph convolutional network algorithm that allows you to capture both the topology of a network as well as useful node attributes.Besides this is an **inductive"" algorithm meaning that it does not need to be trained on whole graphs and can be used for inference on unseen nodes and graphs.

More useful information is available [here](http://snap.stanford.edu/graphsage/) and [here](https://blogs.oracle.com/datascience/graphwise-graph-convolutional-networks-in-pgx)


## 2) Facebook Keynote -  Going beyond supervised learning


Broad classes of Machine Learning outside the classical supervised approaches used at Facebook:

a) Weak Supervision: Use labels are already available in the data e.g. hashtags in Instagram photos. Facebook built the world's best image recognition system using the 3.5 billion images shared on Instagram using the hashtags.

A state of the art model was also created on 65 million videos to recognize 10,000 actions using the available hash tags.

b) Semi Supervised Learning: Use a small amount of labelled data to train a teacher model model and use this model to generate predictions on unlabeled data. Now pre-train a student model on this larger data set and fine tune on the original labelled data.


c) Self Supervised Learning: The best example is Language modelling where you can take a sentence of  length N and use the first N-1 words to predict the Nth word or predict a word in the middle using the surrounding words.

At facebook, self supervised learning based on Generative Adversarial Networks is used to create more robust models that can be used to detect images that have been manipulated by overlaying patterns to get around restrictions such as nudity.

This is done by training a GAN to recover the  original image from a manipulated image.

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/fb_self_supervised_GAN.PNG)

FB takes images, overlays it with random patterns. The generator tries to recreated the original while the discriminator tries to discriminate between the originals and the image generated by the generator. The model minimizes the adversarial loss which captures how well the system can fool the discriminator as well as the perceptual loss which captures how well the generator is able to recreate the original.

After training, just the generator can be used to remove patterns from images.

Another interesting application of this technique at Facebook is **machine translation without labels**.

This relies on creating automatic word dictionaries. First,you create vector representations for vocabularies in each languages and leverage the fact that the vector representations of the same word in different languages share structure to learn a *rotation matrix* to align them and produce a word by word translation.

Then  you build a language model in each language to learn the sentence structure in each language. This language model can then be used to reorder the word by word translation produced in the previous step. This gives you an initial model to translate between the two languages (L1 to L2  and L2 to L1).

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/Language_Model0.PNG)

Now a technique called **back translation** can be used to iteratively improve the model. In this method, you take the original sentence in Language 1 and translate this to Language 2 using the initial *L1 to L2*  model. Now you translate this back to language 1  using the *L2 to L1* model with the original sentence in Language 1 being the target. This will allow you to improve the *L2 to L1* model. By flipping the order of the translations, you can also improve the *L1 to L2* model

![](/post/2020-06-06-oreilly-strata-ai-confrence_files/Language_Model1.PNG)


d) Reinforcement Learning:

Facebook uses RL to recommend notifications to users on Instagram. 

FB also has created a platform called [Habitat](https://aihabitat.org/) 'a research platform for embodied AI agents to operate in real world-like environments and solve tasks like navigation, question-answering, et cetera'.


## 3) Data Science + Design Thinking  - A Perfect blend to achieve the best user experience  | Intuit

Design for delight by
1) Demonstrating deep customer empathy - Know customers deeply by observing them and define problems in human centric terms
2) Go broad to go narrow - Generate lots of ideas before winnowing them. Quantity first then focus on quality.
3) Perform rapid experiments with customers - Rapid prototyping and AB testing


## 4) Explaining Machine Learning Models | Fiddler Labs

Attribution problem : Attribute a prediction to input features. Solving this is the key goals of Explaining ML Models

Naive approaches to do this include:

1) Ablation: Drop each feature and note the change in prediction
2) Feature Gradient: $ x_i \times  \frac {dy}{dx_i}$ where $ x_i $ is feature and $ y $ is the output

**Integrated Gradients** -  This is a technique for attributing a differentiable model's prediction to the input features

A popular method for non-differentiable models is Shapley values. 


Both Integrated Gradients and Shapley Values come with some axiomatic guaranteed.The former uniquely satisfies 6 axioms while the latter uniquely satisfies 4. Side note: [This](https://christophm.github.io/interpretable-ml-book/)
is my go to reference for interpretability techniques.


Example of an axiom is the sensitivity axiom:

> All else being equal, if changing a feature changes the output, then that feature should get an attribution. Similarly if changing a feature does not change the output, it should not get an attribution.

Integrated Gradients is the unique path integral method that satisfies: Sensitivity, Insensitivity, Linearity preservation, Implementation invariance, Completeness and Symmetry


Another problem related to interpretability that remains an open problem for many classes of black box models is 
**Influence** - i.e. Which data points in the training data influenced the model the most.



## 5) Snorkel

Snorkel is a weak supervised learning approach that came out of Stanford. More information available [here](https://www.snorkel.org/)

The key operations in the Snorkel workflow include:

1) Labeling Functions: Heuristics to label data provided by experts
2) Transformation Functions: Data Augmentations
3) Slicing Functions: Partition the data specifying critical subsets where model performance needs to be high

For this approach to work at least 50% of the labeling functions need to be better than random.


## 6) Managing AI Products | Salesforce

To demonstrate value to business stakeholders, which is the ultimate goal of anyone who works in a corporation, it is essential to tie business metrics to model metrics. This should ultimately inform what kind of ML we decide to use.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/acceptance_criteria.PNG)

The figure above demonstrates the accuracy of the model(x-axis) required to provide a material lift in the business metric(y-axis) e.g. conversion rate. If the base line improvement rate in conversion that we need to deliver is only 2%, a model that has accuracy in the range 50 - 75% is sufficient. This means we could rule out sophisticated models like Neural Nets that are harder to deploy and maintain and focus on simpler models that are easier to build or maintain. 


## 7) Explainability and Bias in AI and ML| Institute for Ethical AI and ML

Undesired bias can be split into two conceptual pieces

**1) Statistical Bias (Project Bias)** : The error between where you ARE and where you could get caused by modeling/project decisions

  * Sub optimal choices of accuracy metrics/cost functions
  * Sub optimal choices of ML models chosen for the task
  * Lack of infrastructure required to monitor model performance in production
  * Lack of human in the loop where necessary



**2) A-priori bias (Societal Bias)** : The error between the best you can practically get, and the idealistic best possible scenario - caused by a-priori constraints

  * Sub optimal business objectives
  * Lack of understanding of the project
  * Incomplete resources (data, domain experts etc)
  * Incorrectly labelled data (accident or otherwise)
  * Lack of relevant skill sets
  * Societal shifts in perception
  

Explainability is key to:

a) Identify and evaluate undesirable biases
b) To meet regulatory requirements such as GDPR
c) For compliance of processes
d) To identify and reduce risks (FP vs FN)

**Interpretability != Explainability**

 * Having a model that can be interpreted doesn't mean in can be explained
 * Explainability requires us to go beyond algorithms
 * Undesired bias cannot be tackled without explainability

Library for Explainable AI: [xAI](https://github.com/EthicalML/XAI) [alibi](https://github.com/SeldonIO/alibi)



Anchor points: What are features that influenced a specific prediction for a data instance? This can be evaluated by roughly by pulling out a feature and estimating its impact on the model prediction.

Counterfactual: How would the input/features have to change for the prediction to change?


## 8) Usable Machine Learning - Lessons from Stanford and beyond | Stanford University

- For deep learning, improvement in performance requires exponential increase in data
- Deep learning still doesn't work very well with structured data
- Don't look for a perfect model right out of the gate, instead iterate towards higher quality models
- Measure implicit signals where possible. e.g. Is a user spending time on a page or closing a window


## 9) Human Centered Machine Learning | H2O.ai

- Establish a benchmark using a simple model from which to gauge improvements in accuracy, fairness, interpretability or privacy

-  Overly complicated featured are hard to explain. Features should provide business intuition.(More relevant for regulated industries)

- For fairness, it is important to evaluate if different sub groups of people are being treated differently by your ML model (Disparate Impact). Need to do appropriate data processing. OSS:  [AIF360](https://github.com/IBM/AIF360), [aequitas](https://github.com/dssg/aequitas)

- Model Debugging for Accuracy, Privacy or Security: This involves eliminating errors in model predictions by testing using adversarial examples, explaining residuals, random attacks and what if analysis. Useful OSS: [cleverhans](https://github.com/tensorflow/cleverhans), [pdpbox](https://github.com/SauceCat/PDPbox),
[what-if-tool](https://github.com/pair-code/what-if-tool)


## 10) Operationalizing AI at Scale: From drift detection to monitoring business impact | IBM Watson

Note: See my post on [concept drift](https://www.govindgnair.com/post/detecting-concept-drift/) for a general understanding of the problem.


* Fair Credit Act  requires a sample of model decisions to be explained to the regulator. Same with GDPR.

Watson Openscale provides the following capabilities:

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Watson.PNG)

Important to realize high model performance doesn't always correlate to high business performance. Need to correlate model KPIs and Business KPIs. IBM openscale will automatically correlate model metrics in run time with business event data to inform impact on business KPIs.

Types of concept drift:

1) Class Imbalance. e.g. The imbalance ratio shifts.
2) Novel Class Emergence e.g. A new category of chats in a chatbot
3) Existing Class Fusion e.g. A bank wants to merge the loan approved class with the loan partially approved class

Accuracy Drift: Has model accuracy changes because of changing business dynamics? This is what businesses really care about. This could happen due to:

a) Drop in Accuracy: Model accuracy could drop if there is an increase in transactions similar to those which the model was unable to evaluate correctly in training data

- Use training and test data to learn what kind of data the model is making accurate prediction on and where it is not. You can build a secondary model to predict the model accuracy of your primary model and see if the accuracy falls

b) Drop in data consistency:  Drop in consistency of data at runtime compared to characteristics at running time.

E.g. % of married people applying for your loan has increased from 15% to 80%. These are the kind of explanations that are accessible to a business user.

- Open scale can map a drop in business KPI to the model responsible for the drop and identify the samples of data that has changed.


## 11) Monitoring production ML Systems | DataVisor

Datavizor is a company that specializes in unsupervised ML for fraud detection.

There are potentially several issues that can occur in a production ML systems as shown below. Robust monitoring systems are required to be able to detect and resolve these issues in a timely fashion.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/DataVizor.PNG)

You can react to these issues by having the ability to roll back your system to the previous stable build or by auto scaling to a bigger cluster but it is more cost effective to be able to detect and prevent these issues.

Some approaches to monitoring model quality:

1) Build a surrogate model(offline) to monitor the performance of the deployed model(online)
2) Track model drift
3) Carry out anomaly detection on model outputs and metadata

Anomaly detection using Time series decomposition is a suitable approach.

Additive decomposition of a time series:

$$ Y_t = T_t + S_t + R_t $$

where $ T_t $ is the trend component, $ S_t $ is the seasonal component and $ R_t $ is the residual component.

Subtract the trend and seasonal components from the signal to get the residual component.You should be able to use the residual component to track anomalies.

$$ R_t = Y_t - T_t - S_t $$             

This approach can create unexpected drops in the residual component as shown in red in the image below.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/anomdetection2.PNG)

To resolve this, obtain the residual component by subtracting the median instead of the trend.

The mean absolute deviation (MAD) can then be used to identify anomalies.

$$ If\ Distance\ to\ Median > x \times MAD : anomaly $$


## 12) Reference Architectures for AI and Machine Learning | Microsoft

Distributed training of models can be implemented via data parallelism or model parallelism.

In data parallelism, the entire model is copied to each worker that processes a subset of the total data. The batch size can hence be scaled up to the number of workers you have. Very large batch sizes can cause issues with convergence of the network.

In model parallelism, the model is split across workers and there has to be communication of gradients between the nodes during the forward and backward pass.

Data parallelism is more fault tolerant and common.

* When to use distributed training?
 - Your model us too big to fit a sufficiently large batch size
 - Your data is large
 - Your model requires significant GPU computation

You do not need distributed training if:
 - You want to run hyperparameter tuning
 - Your model is small
 

### Reference Architecture for Distributed Training



![](/post/2020-06-06-2019-oreilly-ai-conference_files/distributed_training.PNG)

Azure ML supports distributed training for TF, Pytorch and Keras. The dependencies are placed in a docker container that runs on the host machine. The storage can be mounted to each of the nodes where training happens.

Azure ML will create the appropriate docker containers and configure the Message Passing interface (MPI) which is essential for distributed training. Azure ML will run the script on the nodes and push the results to blob storage.


### Architecture for Real Time Scoring with DL Models


![](/post/2020-06-06-2019-oreilly-ai-conference_files/realtimescoring.PNG)

- Model served as a REST endpoint
- Should handle requests from multiple clients and respond near instantaneously
- Timing of requests are unknown
- Solution should have low latency, scalable and elastic to seasonal variations

Best practices for deploying an image classification system:

- Use GPU for efficient inferencing (faster than CPU)
- Send multiple images with a single request (GPUs process batches more efficiently)
- Use HTTP 2.0 (allows you to accept multiple request)
- Send image as file within HTTP request

### Architecture for Batch Scoring with DL Models

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Batchscoring.PNG)

* Scoring can be triggered (by appearance of new data) or scheduled (at recurring intervals)
* Large scale jobs that run asynchronously
* Optimize for cost, wall clock time and scalability

The way Azure implements this is given [here](https://github.com/Azure/batch-scoring-for-dl-model)


## 13) Semi Supervised Learning for ML | Rocket ML

Three important considerations:

1) Total cost - May not be possible to acquire the volume of labels needed to build a good model
2) Social Accountability - Interpretability, Explainability, Traceability
3) Robustness - Should not be vulnerable to to easy adversarial attacks

![](/post/2020-06-06-2019-oreilly-ai-conference_files/rocketml1.PNG)



### Anomaly Detection Problem

* Labels are rare and expensive to acquire

Types of Anomalies:

1) Point Anomalies : E.g. a point In a time series
2) Contextual Anomalies : Anomalous in a given context but not in another
3) Collective Anomalies: Group of points constitute and anomaly


* KNN is parameterized by the no of neighbors(K) and the distance metric.

* In an unsupervised setting, use the [Local outlier Factor](https://en.wikipedia.org/wiki/Local_outlier_factor) to identify outliers.

#### Anomaly detection performance

* Build KNN for different values of K
* Compute LOF for the neighboring k points
* Use threshold on LOF to determine anomaly vs normal


* As k increases the performance of the model increases. This is because for small k, by looking at really close neighbors, density is not too different and hence anomalies are not found. i.e. you miss the forest for the trees.
For larger k, by looking beyond the clusters into normal areas, density differences stand out.

* Possible methods to generate better features: Matrix Factorization, Pre-trained models, Auto Encoders.
* Reducing dimensionality using SVD can improve accuracy by addressing the curse of dimensionality problem

* KNN is computationally intensive so need highly efficient, parallelized implementations for this approach to work.


## 14) Sequence to Sequence Learning for Time Series Forecasting | Anodot


Structural Characteristics that affect the choice and performance of your Time Series forecasting algorithm:

![](/post/2020-06-06-2019-oreilly-ai-conference_files/TimeSeries1.PNG)

Existing techniques are unable to address situations when many of these structural characteristics co-occur.

Key development that allowed the application of Neural Nets to time Series: Recurrent Neural Nets and Back Propagation Through Time

RNNs can be memory based (GRU and LSTM) or attention based (Transformers).

**Considerations for getting accurate TS forecasts:**
1. Discovering influencing metrics and events
 - Look at correlations between target and time series features
 
2. Ensemble of models - Usually require multiple algorithms

3. Identify and account for unexplainable data anomalies
 - Identify anomalies and use this to create new features
 - Enhance anomalies that can be explained by external factors
 - Weight down anomalies that can't be explained by external factors
 
 

4. Identify and account for different time series behaviors
 - Training a single model for multiple time series does not work if each series shows a different seasonality. 
   Difference can be in frequency or strength.
 - Mixing stationary and non stationary time series also does not work


## 15) Transfer Learning NLP: Machine Reading comprehension for question answering | Microsoft

Attention can be content based or location based. Question Answering requires content based attention.

Machine Reading Comprehension systems should be capable of summarizing key points in a piece of texts, it can answer questions and also do reply (e.g Gmail auto suggestion)and comment.

Machine reading comprises (in increasing order of complexity) Extraction, Synthesis & Generation and Reasoning & Inference.

Open Source datasets available: SQUAD (Stanford) and Marco (Microsoft)

Best performing algorithms:

For extraction: BIDAF(for small paragraphs) , DOCQA(large documents), S-NET (small multiple paragraphs)

For reasoning and inference: SynNet ( multiple small paragraphs) and Open NMT (multiple small or large paragraphs)

BIDAF: Bi Direction Attention Flow for Machine Comprehension


## 16) Applying AI to secure the Payments Ecosystem | Visa Research

Data Exfiltration: Unauthorized copying, transfer or retrieval of data

Merchant Data Breach: Occurs when an unauthorized part accesses a merchant's network and steals cardholder data. Cardholder data is later used to commit fraud. 

Once a breach occurs, the criminals may enrich the data they have collected and then bundle and resell it on the dark web.

Visa's goal is create an AI based solution to detect such breaches as early as possible with high accuracy and recall.


### Solution

Visa uses a semi supervised CNN for label augmentation given incidences of data breaches are  extremely rare. For detection or classification of breaches a supervised CNN + a Deep Neural Net is used.

For each merchant, a table is created recording the first instance a card is used at a merchant along with subsequent transactions at the same merchant with any incidence of fraud( the fraud may have happened at the same or different merchant). This table is transformed into an image which captures the same information.


![](/post/2020-06-06-2019-oreilly-ai-conference_files/visa1.PNG)

In instances where no breach occurred, the image would show authorizations(green) and fraud(red) being distributed across the image whereas for instances where breach occurred, there would be a clear separation between the two.

A CNN is trained on the available labelled data to extract features that are indicative of a data breach. This model is used as a 'feature classifier' (in image below) to label the unlabeled data. The enriched data with labels is then used to train a second classifier to actually detect fraud. 

The second classifier uses more near time features to enable early detection of breaches.Time series features are prominently used. Multi channel time series that combine multiple time series are created and this is fed into a CNN, The features extracted by the CNN along with more static features such as type of merchant or location of merchant are fed into a deep neural network with the labels being those generated by the first CNN.


![](/post/2020-06-06-2019-oreilly-ai-conference_files/visa2.PNG)


## 17) Generative Models for fixing image defects| Adobe

Traditional approach is to manually retouch the image. Auto tune functions exist that can enhance global features such as exposure, saturation and lighting but not local features (e.g. color of specific objects in an image.)

### Popular GANS

1) Style GAN for Face generation at NVIDIA
2) Cycle GAN for Style Transfer at UC Berkeley
3) Dual GAN
4) Disco GAN

Neural Style Transfer: Transform a given image in the style of a reference(domain) image.This uses only two images.It does not require paired image - a single image of the domain e.g. an art piece is required.

Style Transfer (with GAN):  Uses a collection of images. E.g. 500 image in domain A and 500 images in domain B. This again does not require paired data.

The benefit of this approach is that it does not require paired data i.e. the pre and post image of the same object.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/adobe1.PNG)



### GAN based Image Enhancer


![](/post/2020-06-06-2019-oreilly-ai-conference_files/adobe2.PNG)



There are two generators and discriminators, hence the name DUAL GAN.
Learning to create a superior image G2(Defective) is difficult, hence the system is trained to optimize G1(G2(defective)) - this is the cycle in Cycle GAN.

The UNET segmentation model helps to learn which part of the image is a tree, sky or some specific object. The regions identified by the UNET model are then locally enhanced.



### Challenges

* Weak features
* Subjective, Noisy, Mislabeled Data - Humans determine whether an image is good or not
* Small Dataset
* Batch size used is smaller given there are four models to train, hence harder for models to converge
* Training GANs is hard and time consuming
* GAN inference is time consuming and does not scale


### GAN Model Evaluation

Creating ground truth labels is a manual process requiring an artists to retouch the images, this is not feasible.
In the absence of ground truth labels:

* Train Discriminative models (VGG or Resnet) on good and bad images. Score of the model is the metric.


## 18) Supercharging business decisions with AI | Uber

The finance planning department at Uber carries out rolling 12 month forecasts for Uber trips. Some of the challenges involved here are:

* Time series of trips vary considerably across cities based on economy, geography, quality of public transport etc.
* Shape of time series can change over time. It can grow rapidly initially but then flatten
* Affected by holidays,e vents, weather etc.
* Newly launched cities have little or no data

### Planning

Below is a planning model used by Uber. The goals of the first part of the system here is to generate cost curves that track the relationship between money spent on driver and rider promotions and no of sign ups. the goals is to find the optimal point maximizing ROI.

The levers available are:

$ \\$_{Ref} \rightarrow $  Dollars spent on Referrals <br />
$ \\$_{DPaid} \rightarrow $ Dollars paid to drivers <br />
$ \\$_{Promo} \rightarrow $ Dollars paid in promotions to riders <br />
$ SU_D \rightarrow $ Sign Ups from drivers <br />
$ SU_R \rightarrow $ Sign Ups from riders

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Uber0.PNG)



The second part of the system is the **trips models**

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Uber1.PNG)

$ FT_D \rightarrow $ First trip per rider <br />
$ FT_R \rightarrow $ First trip per driver <br />
$ RR_D \rightarrow $ Retention Rate pf drivers <br />
$ RR_R \rightarrow $ Retentions rate of riders <br />
$ TPA_D \rightarrow $ Trips per active driver <br />
$ TPA_R \rightarrow $ Trips per active rider <br />
$ RES_D \rightarrow $ Resurrected drivers <br />
$ RESR_R \rightarrow $ Resurrected Riders

Active drivers are those that are active at least once per month.

This a classic funnel. Promotions lead to sign ups and first rides, but many churn at this point looking for new promotions.

This variables are used to calculate the no of trips, active riders, active drivers, resurrected drivers, resurrected riders and per trip metrics.

Resurrected riders /drivers are those who haven't been active in the previous three months. 


### Forecasting Models

Riders and Drivers are cohorted based on month of joining for each city.

For each cohort, three models as shown below are used. A black box model ensembles these three models by assigning weights to the predictions of each. Evaluation Metric is MAPE and SMAPE.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/Uber2.PNG)


Models used include ETS, ARIMA and TBATS.

Model averaging is done at different training end points to correct for misleading volatility in recent data points.


### Modelling Seasonality

Seasonal holidays can shift from year to year that can cause problems. Uber uses the a Day to Month model to account for this. 

Uber  uses FB's prophet and python holiday library.

The sophistication of the systems used by Uber's financial planning team is truly remarkable. There was a lot more content in this talk that I didn't fully follow given my familiarity with this domain is limited.


## 19) An age of embeddings| USC Information Sciences Institute

> 'You shall know a word by the company it keeps' - JR Firth(1957)


### Word embeddings

Skip Gram: Take a word and predict the surrounding words
CBOW: Take surrounding words and predict the target word

Glove word embeddings are based on matrix factorization of a matrix with entries corresponding to frequency of their co-occurence.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/embeddings1.PNG)

TFIDF is a simple type of an embedding, but they are sparse adn high dimensional. 



### Image Embeddings

Can use fully connected layers before the softmax layer of a CNN built for classification. Auto encoders are also a popular choice to create embeddings.


### Document Embeddings

Topic models give lower dimensional embeddings of documents.

Below is a way to get document embeddings from a word embeddings.$d_{test}$ in the image below is the id for a document inserted as  a word into the document.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/embeddings2.PNG)

### Graph and Network Embedding

Graph CNNs can be used to create embeddings.


# 20) Recommendation system challenges at Twitter scale | Twitter

Recommendation systems can be content based, collaborative filtering driven or a hybrid of the two.

* Given twitter has user follows, collaborative filtering is a more natural approach
* Content based approaches are more challenging  given short document length, multi lingual content and multiple languages within the same tweet.
* Recommendations served include 
- user-user: Who to follow
- user-item: Home timeline with tweets. Which tweet  to follow 
- item-item : Recommend new trends/tweets etc.

An item at twitter can be tweets,events,trends,moments or live video/broadcast.

Twitter faces a unique challenge that the shelf life of a tweet is typically only a few hours unlike movies or e-commerce products that have a shelf life of months or years. People use twitter to keep up with the most current information.


This figure provides a framework for thinking about recommender systems.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/twitter1.PNG)

1) Small Number of items + Long Shelf Life: Ideal spot to be in
2) Small Number of items + Short Shelf Life: Can't have a cold start problem because items disappear quickly
3) Large Number of items + Long Shelf Life: A doable problem
4) Large Number of items + Short Shelf Life : The hardest problem


Question: What to optimize recommendations for? Can be retweets, likes, CTR, DAU/MAU etc. Optimizing too hard any of these may not be appropriate. Optimizing for clicks promotes click baity content, while optimizing for engagement may reduce diversity. 

* Recommendations at twitter do not assign higher weights to a tweet from a verified user

### Challenges in User - Item Recommendations

* Distribution of content and user interests keep changing over time so the recommender system is always predicting out of distribution.

* Content keeps changing and user's interests keep changing. 

* Interests can be long term, short term (e.g. elections) or geographic


Twitter is trying to map users and items into more stable spaces where learning can happen.

Co-occurrence of entities yields information about topics. e.g. LeCun + CNN vs White House + CNN


## 21) PyTorch at Scale for translation and NLP | Facebook

Common NLP Tasks:

* Classification
* Word Tagging E.g. Part of Speech
* Sequence Classification
* Sequence to Sequence E.g. Translation

Model should be able to take additional input features such as relevant metadata

### Inference

* Python is not good for inference. Scaling up to multiple CPUs is often necessary for inference which is challenging in Python due to the [global interpreter lock](https://docs.python.org/3/c-api/init.html#thread-state-and-the-global-interpreter-lock)

* Saving models by saving the weights as in TF or PyTorch is not very resilient as you need to reconstruct the model and reload the weights for  for inference. Small changes such as change in internal variable names can break the model. ONNX and TorchScript are resilient model formats for PyTorch.

* If models are of  reasonable  size, they can be duplicated in multiple replicas and scaled up according to traffic needs.
* If models are very large, you need intra model parallelism or sharding. This might be necessary if vocabularies are very large.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/pytext1.PNG)


An alternative to sharding is BPE (Byte Pair Encoding). You look over a corpus of text and learn sub word units and tokenize into these sub word units. This reduces the vocabulary size and can work across languages and hence is a good choice for large multilingual models.

### Training

A training framework needs:
* Data
* Model
* Metrics
* Trainer - Might make sense to fold this into the model in some cases. e.g. PyTorch ignite


![](/post/2020-06-06-2019-oreilly-ai-conference_files/pytext2.PNG)



Training a Classifier in Pytext comprises the following steps

1) Get data
2) Tokenize and Numericalize
3) Convert to Tensor and pad to equalize lengths
4) Get  Model outputs
5) Compute Loss and Optimize using Backprop


## 22) Turning AI research into a revenue engine | Verta.ai

ModelDB: Model Life Cycle Management System built at MIT. Tracks hyperparameters, metrics and other model metadata

You need to be agile with ML/AI for it to make a revenue impact with ML. The new model lifecyle is


![](/post/2020-06-06-2019-oreilly-ai-conference_files/vertaai1.PNG)

Agile principles for ML:

1) Good enough vs best models
2) Constant iteration: Build, deploy, repeat
3) Monitoring: gather feedback and repeat


The challenges Verta focuses on tackling are in the lower half of the model lifecycle:

1)How do you run and manage ML Experiments? Need a git equivalent for ML models
2)Deploying Models in Production
3)Monitoring Model Performance

* Model versioning requires code versioning, data versioning, config versioning and environment versioning

* Automated Model deployment for models equivalent to Jenkins is missing. Automated Monitoring and Collaboration are also required

Verta's solution includes:


![](/post/2020-06-06-2019-oreilly-ai-conference_files/vertaai2.PNG)

Auto scaling is accomplished through containers and Kubernetes

For a sales engineering client, it resulted in the following improvements.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/vertaai3.PNG)



## 23) Deep Learning Applications in NLP and Conversational AI | Uber

The right problem to tackle with AI
* Involves decision making under uncertainty
* Within reach of AI Tech
* Touches customer pain point
* Delivers business value

The best model depends on data available and the data required depends on task complexity.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/ubernlp1.PNG)

Uber has deployed ML systems for their customer support platform. Based on the customer questions,the system recommends templates to the customer service agent for replies.

Uber also has developed one click chat reply templates for drivers.This is similar to Gmail's auto reply features.
The major challenge here is that the chats are often informal and have lots of typos. However, the task complexity is lower compared to Gmail as the variety of conversations is lower.

To solve this Uber has used a two step algorithm.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/ubernlp2.PNG)

Given the types of responses are limited depending on the intent of the question, intent detection is the primary focus.

Intent detection is carried out as follows

1. Train Doc2vec model to get dense embedding for each message (Self Supervised Learning)
2. Map labelled data to embedding space (You should have labelled data giving intent of various messages)
3. Calculate centroid of each intent cluster
4. Calculate distance between incoming message and intent cluster centroid
5. Classify into Nearest Neighbor Intent Cluster.


Another use case allows Uber drivers to converse with the Uber App without touching their phones.

Conversational AI can be done with two approaches as shown below. The first one uses a modular approach with different modules carrying out specific sub tasks while the second uses an end  to end model.

![](/post/2020-06-06-2019-oreilly-ai-conference_files/ubernlp3.PNG)


For task oriented conversations, the first is preferred.

Uber also has created a system that combines the strengths of Spark which is CPU driven and Deep Learning frameworks such as Tensorflow that rely on GPUs.

The pre-processing of data is done on Spark Clusters and are transferred to GPU clusters where the models are trained.

For inference - Apache Spark and Java are used for both batch and real time requests. The tensorflow model is converted into a spark transformer in a Spark pipeline.











































































































































