#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Set epsilon value conditional on whether we are training X or O
#  eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
#
# #Lists to keep track of rewards earned by both players during training
#
# rewards_X = []
# rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
#
#
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
#Lists to keep track of rewards earned by both players during training
#
# rewards_X = []
# rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
#
#
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
#
#
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
if train_X:
X_action_type = 'eps_greedy'
else:
X_action_type = 'greedy'
if is_random:
X_action_type = 'eps_greedy'
if train_O:
O_action_type = 'eps_greedy'
else:
O_action_type = 'greedy'
if is_random:
O_action_type = 'eps_greedy'
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
reticulate::repl_python()
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
train_X()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
