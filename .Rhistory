#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
#
#
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
if train_X:
X_action_type = 'eps_greedy'
else:
X_action_type = 'greedy'
if is_random:
X_action_type = 'eps_greedy'
if train_O:
O_action_type = 'eps_greedy'
else:
O_action_type = 'greedy'
if is_random:
O_action_type = 'eps_greedy'
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
quit
reticulate::repl_python()
def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):
"""
Function to train two players in a game of tic-tac-toe
Arguments:
n_games: Number of games on which tot rain
is_random: should actions of untrained agent be random or deterministic according to Q table
"""
# If Q is not provided, randomize intially, if provided, it will be used to select actions greedily
#Set epsilon value conditional on whether we are training X or O
reticulate::repl_python()
eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0
if "Q_X" in kwargs:
action_type_X = "greedy"
assert train_X == False ,"Train flag should be set to False if Q table is being provided"
Q_X = kwargs["Q_X"]
else:
Q_X = initialize_Q(States_X)
if "Q_O" in kwargs:
action_type_O = "greedy"
assert train_O == False ,"Train flag should be set to False if Q table is being provided"
Q_O = kwargs["Q_O"]
else:
Q_O = initialize_Q(States_O)
#Lists to keep track of rewards earned by both players during training
rewards_X = []
rewards_O = []
#
# if train_X:
#   X_action_type = 'eps_greedy'
# else:
#   X_action_type = 'greedy'
#   if is_random:
#     X_action_type = 'eps_greedy'
#
#
# if train_O:
#   O_action_type = 'eps_greedy'
# else:
#   O_action_type = 'greedy'
#   if is_random:
#     O_action_type = 'eps_greedy'
# for i in tqdm(range(n_games),position=0,leave=True):
#   eps = 0.05*0.99**i
#
#   t_board_X.reset_board()
#   t_board_O.reset_board()
#
#   #X lands on empty board
#   S_X = t_board_X.board_to_state()
#
#   #X plays first
#   eps = eps_(train_X,i)
#
#
#   x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)
#   x_action1d = t_board_X.b2_to_s1[x_action]
#
#   R_X = t_board_X.my_move(x_action) # make move on X's board
#   t_board_O.opponent_move(x_action) # make same move on O's board
#
#   while not (t_board_X.is_game_over() or t_board_O.is_game_over()):
#     S_O = t_board_O.board_to_state()
#
#     #O plays second
#     eps = eps_(train_O,i)
#
#
#     o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)
#     o_action1d = t_board_O.b2_to_s1[o_action]
#     R_O = t_board_O.my_move(o_action) #make move on O's board
#     t_board_X.opponent_move(o_action) #make same move on X's board
#
#     if t_board_O.is_game_over():
#       #need to end game here if O makes the winnng move and add a reward
#       if train_O:
#         Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0
#       if train_X:
#         #Need to penalize X's previous action if game is over
#         Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d])
#
#       rewards_O.append(R_O)
#       rewards_X.append(-R_O)
#       break
#
#     S_X_new = t_board_X.board_to_state() #Get new state
#     #Calculate max_a Q_X(S',a)
#     if train_X:
#       x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new
#       x_action_1d = t_board_X.b2_to_s1[x_action_]
#       Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])
#
#     S_X = S_X_new
#
train_X()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
install.packages("xfun")
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::update_meta_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
tanks_df <- data.frame(
'Month' = c("June 1940", "June 1941", "August 1942"),
"Statistical Estimate" = c(169,244,327)
)
View(tanks_df)
blogdown:::serve_site()
?kable
knitr::opts_current$get('label')
getwd()
setwd("~/My_Website/content/post")
blogdown:::serve_site()
setwd("~/My_Website")
blogdown:::serve_site()
blogdown:::serve_site()
library(ggplot2)
library(dplyr)
# Construct a full data frame for plotting
df <- data.frame(cbind(c(1:(ncol(ci))),N_hat,t(ci)))
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
plot(c(1:10),c(1:10))
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
posterios_prob <- function(prior,sensitivity,specificity){
post <- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))
return(post)
}
posterior_prob <- function(prior,sensitivity,specificity){
post <- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))
return(post)
}
posterior_prob(0.01,0.99,0.9)
posterior_prob(0.25,0.99,0.9)
cat("Probability of Bob having Covid is:",round(posterior_prob(0.01,0.99,0.9),2))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9)))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9),2))
posterior_prob <- function(prior,sensitivity,specificity){
post <- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))
return(post)
}
cat("Probability of Bob having Covid is:",round(posterior_prob(0.01,0.99,0.9),2))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9),2))
((1/12e6)*(4999999/5e6))/((1/12e6)*(4999999/5e6) + (1/5e6))
?kable
??kable
getwd()
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair"),"Probability" = c("1/4","1/3"))
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair"),"Probability" = c("1/4","1/3"))
kable(df)
knitr::kable(head(mtcars[, 1:4]), "pipe")
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair"),"Probability" = c("1/4","1/3"))
kable(df)
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair"),"Probability" = c("1/4","1/3"))
kable(df)
?kable
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair","Woman with pony tail", "African American man with beard","Interracial couple in a car","partly yellow car"),"Probability" = c("1/4","1/3","1/10","1/10","1/1000,1/10"))
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair","Woman with pony tail", "African American man with beard","Interracial couple in a car"," Partly yellow car"),"Probability" = c("1/4","1/3","1/10","1/10","1/1000","1/10"))
kable(df,caption="Probability of seeing a match with characteristics seen by Eye witness ")
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair","Woman with pony tail", "African American man with beard","Interracial couple in a car"," Partly yellow car"),"Probability" = c("1/4","1/3","1/10","1/10","1/1000","1/10"))
kable(df,caption="Probability of seeing a match with characteristics seen by Eye witness ")
0.01/0.99
round(bf(0.99,0.9),2)
bf<- function(sensitivity,specificity){
return(sensitivity/(1 - specificity))
}
round(bf(0.99,0.9),2)
cat(" the odds of Bob having Covid is:",round((0.01/0.99)*bf(0.99,0.9),2))
cat("Probability of Bob having Covid is:",round(posterior_prob(0.01,0.99,0.9),2))
posterior_prob <- function(prior,sensitivity,specificity){
post <- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))
return(post)
}
cat("Probability of Bob having Covid is:",round(posterior_prob(0.01,0.99,0.9),2))
cat(" The odds of Alice having Covid is:",round((0.25/0.75)*bf(0.99,0.9),2))
odds_to_prob <- function(o) return(o/(1+o))
cat("Probability of Bob having Covid is:",round(odds_to_prob(0.1),2))
cat("Probability of Bob having Covid is:",round(odds_to_prob(3.3),2))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9),2))
prob_to_odds <- function(p) p/(1-p)
prob_to_odds(0.01)
prob_to_odds(0.25)
odds_to_prob(prob_to_odds(1/1000000)* bf(0.81,0.63))
odds_to_prob(prob_to_odds(1/100000)* bf(0.81,0.63))
odds_to_prob(prob_to_odds(1/100000)* bf(0.91,0.9))
odds_to_prob(prob_to_odds(1/100)* bf(0.91,0.9))
odds_to_prob(prob_to_odds(1/1000)* bf(0.91,0.9))
odds_to_prob(prob_to_odds(1/1000000)* bf(0.81,0.63)^3)
odds_to_prob(prob_to_odds(1/1000)* bf(0.81,0.63)^3)
posterior_prob <- function(prior,sensitivity,specificity){
post <- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))
return(post)
}
cat("Probability of Bob having Covid is:",round(posterior_prob(0.01,0.99,0.9),2))
cat("Probability of Alice having Covid is:",round(posterior_prob(0.25,0.99,0.9),2))
prob_to_odds <- function(p) p/(1-p)
prob_to_odds(0.01)
prob_to_odds(0.25)
bf<- function(sensitivity,specificity){
return(sensitivity/(1 - specificity))
}
round(bf(0.99,0.9),2)
cat(" The odds of Bob having Covid is:",round((0.01/0.99)*bf(0.99,0.9),2))
cat(" The odds of Alice having Covid is:",round((0.25/0.75)*bf(0.99,0.9),2))
odds_to_prob <- function(o) return(o/(1+o))
cat("Probability of Bob having Covid is:",round(odds_to_prob(0.1),2))
cat("Probability of Alice having Covid is:",round(odds_to_prob(3.3),2))
library(knitr)
df <- data.frame("Observation" = c("Man with mustache", "Woman with blonde hair","Woman with pony tail", "African American man with beard","Interracial couple in a car"," Partly yellow car"),"Probability" = c("1/4","1/3","1/10","1/10","1/1000","1/10"))
kable(df,caption="Probability of seeing a match with characteristics seen by Eye witness ")
df <- data.frame(" Scenario" = c(" Scenario A", " Scenario B","Scenario C"),"Sensitivity" = c(0.81,0.81,0.81), "Specificity" = c(0.63,0.63,0.63) )
kable(df,caption="Accuracy of scenarios ")
df <- data.frame("Scenario" = c("Scenario A", "Scenario B","Scenario C"),"Sensitivity" = c(0.81,0.81,0.81), "Specificity" = c(0.63,0.63,0.63) )
kable(df,caption="Accuracy of scenarios ")
odds_to_prob(prob_to_odds(1/1000000)* bf(0.81,0.63))
odds_to_prob(prob_to_odds(1/1000000)* bf(0.81,0.37))
cat(" The odds of Bob having Covid is:",round(prob_to_odds(0.01)*bf(0.99,0.9),2))
cat(" The odds of Alice having Covid is:",round(prob_to_odds(0.25)*bf(0.99,0.9),2))
odds_to_prob(prob_to_odds(1/1000)* bf(0.81,0.37))
odds_to_prob(prob_to_odds(1/1000000)* bf(0.81,0.37)^3)
bf(0.81,0.37)
odds_to_prob(prob_to_odds(1/1000)* bf(0.81,0.37)^3)
round(1.285714e-06,6)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
getwd()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)
library(knitr)
knitr::include_graphics('/post/2024-07-09-explanations-causality-and-theories-an-aml-perspective/images/dag2.png',error = FALSE)
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
getwd()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::update_meta_addin()
(36/52)^c(1:36)
sum((36/52)^c(1:36))
sum((36/52)^c(1:36))*(4/52)
c(0:36)
(36/52)^c(0:36)
sum((36/52)^c(0:36))
sum((36/52)^c(0:36))*(4/52)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::update_meta_addin()
